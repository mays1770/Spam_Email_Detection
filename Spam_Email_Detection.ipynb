{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief Summary: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 Data Preparation: the attributes can be classified into two groups and they are generally in the same form. Thus, I decided to not try too complicated data cleaning methods since it is unnecessary. Secondly, I created a scoring called avg_cost_score to measure the average misclassification cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 Modeling with original data: I tried 9 models and used GridSearchCV to find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 Modeling with preprocessed data: I conducted min-max transformation firstly and then use chi-square test to choose 40 attributes. I tried 10 models and used GridSearchCV to find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 Model Evaluation: Looking at the results (see appendix), there are two models that both have very good performance according to accuracy rate. Then I looked at precision, recall, f-measure, ROC curve and AUC to choose the best one. If we choose the best model based on average classification cost, only one shows the best performance, but I also look at other metrics to ensure it doesnâ€™t have other potential flaws. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset and have a look on dimension of dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv('C:/_Emory/1_676-Machine Learning II/Assignments/HW3/spambase.data',header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9     10    11  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.64   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  0.21  0.79   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  0.38  0.45   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31   \n",
       "\n",
       "     12    13    14    15    16    17    18    19    20   21    22    23   24  \\\n",
       "0  0.00  0.00  0.00  0.32  0.00  1.29  1.93  0.00  0.96  0.0  0.00  0.00  0.0   \n",
       "1  0.65  0.21  0.14  0.14  0.07  0.28  3.47  0.00  1.59  0.0  0.43  0.43  0.0   \n",
       "2  0.12  0.00  1.75  0.06  0.06  1.03  1.36  0.32  0.51  0.0  1.16  0.06  0.0   \n",
       "3  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "4  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0   \n",
       "\n",
       "    25   26   27   28   29   30   31   32   33   34   35    36   37   38  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0   \n",
       "\n",
       "     39   40   41    42   43    44    45   46   47    48     49   50     51  \\\n",
       "0  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.000  0.0  0.778   \n",
       "1  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.132  0.0  0.372   \n",
       "2  0.06  0.0  0.0  0.12  0.0  0.06  0.06  0.0  0.0  0.01  0.143  0.0  0.276   \n",
       "3  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.137  0.0  0.137   \n",
       "4  0.00  0.0  0.0  0.00  0.0  0.00  0.00  0.0  0.0  0.00  0.135  0.0  0.135   \n",
       "\n",
       "      52     53     54   55    56  57  \n",
       "0  0.000  0.000  3.756   61   278   1  \n",
       "1  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.000  0.000  3.537   40   191   1  \n",
       "4  0.000  0.000  3.537   40   191   1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at first five rows\n",
    "pd.options.display.max_columns = None \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on whether missing values exist\n",
    "sum(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.059824</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>1.662100</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.809761</td>\n",
       "      <td>0.121202</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>0.094269</td>\n",
       "      <td>0.549504</td>\n",
       "      <td>0.265384</td>\n",
       "      <td>0.767305</td>\n",
       "      <td>0.124845</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>0.102852</td>\n",
       "      <td>0.064753</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>0.097229</td>\n",
       "      <td>0.047835</td>\n",
       "      <td>0.105412</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.136953</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>0.064834</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.132339</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.079196</td>\n",
       "      <td>0.301224</td>\n",
       "      <td>0.179824</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>0.201545</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.335184</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>0.444055</td>\n",
       "      <td>0.531122</td>\n",
       "      <td>1.775481</td>\n",
       "      <td>0.509767</td>\n",
       "      <td>1.200810</td>\n",
       "      <td>1.025756</td>\n",
       "      <td>0.350286</td>\n",
       "      <td>0.442636</td>\n",
       "      <td>1.671349</td>\n",
       "      <td>0.886955</td>\n",
       "      <td>3.367292</td>\n",
       "      <td>0.538576</td>\n",
       "      <td>0.593327</td>\n",
       "      <td>0.456682</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>0.328559</td>\n",
       "      <td>0.555907</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.402623</td>\n",
       "      <td>0.423451</td>\n",
       "      <td>0.220651</td>\n",
       "      <td>0.434672</td>\n",
       "      <td>0.349916</td>\n",
       "      <td>0.361205</td>\n",
       "      <td>0.766819</td>\n",
       "      <td>0.223812</td>\n",
       "      <td>0.621976</td>\n",
       "      <td>1.011687</td>\n",
       "      <td>0.911119</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.285735</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>16.660000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.690000</td>\n",
       "      <td>6.890000</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>22.050000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "                10           11           12           13           14  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.059824     0.541702     0.093930     0.058626     0.049205   \n",
       "std       0.201545     0.861698     0.301036     0.335184     0.258843   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.100000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.800000     0.000000     0.000000     0.000000   \n",
       "max       2.610000     9.670000     5.550000    10.000000     4.410000   \n",
       "\n",
       "                15           16           17           18           19  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.248848     0.142586     0.184745     1.662100     0.085577   \n",
       "std       0.825792     0.444055     0.531122     1.775481     0.509767   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     1.310000     0.000000   \n",
       "75%       0.100000     0.000000     0.000000     2.640000     0.000000   \n",
       "max      20.000000     7.140000     9.090000    18.750000    18.180000   \n",
       "\n",
       "                20           21           22           23           24  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.809761     0.121202     0.101645     0.094269     0.549504   \n",
       "std       1.200810     1.025756     0.350286     0.442636     1.671349   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.220000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.270000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      11.110000    17.100000     5.450000    12.500000    20.830000   \n",
       "\n",
       "                25           26           27           28           29  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.265384     0.767305     0.124845     0.098915     0.102852   \n",
       "std       0.886955     3.367292     0.538576     0.593327     0.456682   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      16.660000    33.330000     9.090000    14.280000     5.880000   \n",
       "\n",
       "                30           31           32           33           34  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.064753     0.047048     0.097229     0.047835     0.105412   \n",
       "std       0.403393     0.328559     0.555907     0.329445     0.532260   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      12.500000     4.760000    18.180000     4.760000    20.000000   \n",
       "\n",
       "                35           36           37           38           39  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.097477     0.136953     0.013201     0.078629     0.064834   \n",
       "std       0.402623     0.423451     0.220651     0.434672     0.349916   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       7.690000     6.890000     8.330000    11.110000     4.760000   \n",
       "\n",
       "                40           41           42           43           44  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.043667     0.132339     0.046099     0.079196     0.301224   \n",
       "std       0.361205     0.766819     0.223812     0.621976     1.011687   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.110000   \n",
       "max       7.140000    14.280000     3.570000    20.000000    21.420000   \n",
       "\n",
       "                45           46           47           48           49  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.179824     0.005444     0.031869     0.038575     0.139030   \n",
       "std       0.911119     0.076274     0.285735     0.243471     0.270355   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.065000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.188000   \n",
       "max      22.050000     2.170000    10.000000     4.385000     9.752000   \n",
       "\n",
       "                50           51           52           53           54  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238     5.191515   \n",
       "std       0.109394     0.815672     0.245882     0.429342    31.729449   \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     1.588000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     2.276000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000     3.706000   \n",
       "max       4.081000    32.478000     6.003000    19.829000  1102.500000   \n",
       "\n",
       "                55            56           57  \n",
       "count  4601.000000   4601.000000  4601.000000  \n",
       "mean     52.172789    283.289285     0.394045  \n",
       "std     194.891310    606.347851     0.488698  \n",
       "min       1.000000      1.000000     0.000000  \n",
       "25%       6.000000     35.000000     0.000000  \n",
       "50%      15.000000     95.000000     0.000000  \n",
       "75%      43.000000    266.000000     1.000000  \n",
       "max    9989.000000  15841.000000     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at basic statisitcs of each attribute\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup metric: average misclassification cost \n",
    "# In this case, I believe classifying a non-spam message as spam will be costlier, since users may miss that email because of the wrong classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def avg_cost(y_true, y_pred):\n",
    "    result=confusion_matrix(y_true, y_pred)[0,1]/confusion_matrix(y_true, y_pred).sum()*10+confusion_matrix(y_true, y_pred)[1,0]/confusion_matrix(y_true, y_pred).sum()*1\n",
    "    return result\n",
    "\n",
    "from sklearn.metrics import  make_scorer\n",
    "avg_cost_score = make_scorer(avg_cost, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X ,y\n",
    "y = df.iloc[:,-1]\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "Optimal Estimator:  DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
      "            splitter='best')\n",
      "Accuracy:  0.9197101449275362\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "gs_tree = GridSearchCV(estimator=DecisionTreeClassifier(random_state=29),\n",
    "                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], 'criterion':['gini','entropy'], \n",
    "                              'min_samples_leaf':[1,2,3,4,5],\n",
    "                              'min_samples_split':[2,3,4,5]}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_tree = gs_tree.fit(X_train, y_train)\n",
    "print(gs_tree.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_tree.best_estimator_)\n",
    "print(\"Accuracy: \", gs_tree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9288"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "from sklearn.metrics import accuracy_score\n",
    "gs_tree_pred = gs_tree.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_tree_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "Optimal Estimator:  DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
      "            splitter='best')\n",
      "AvgCost:  -0.3046376811594203\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "gs_tree2 = GridSearchCV(estimator=DecisionTreeClassifier(random_state=29),\n",
    "                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], 'criterion':['gini','entropy'], \n",
    "                              'min_samples_leaf':[1,2,3,4,5],\n",
    "                              'min_samples_split':[2,3,4,5]}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_tree2 = gs_tree2.fit(X_train, y_train)\n",
    "print(gs_tree2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_tree2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_tree2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2919"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "gs_tree_pred2 = gs_tree2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_tree_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we should firstly transform data before conducting kNN, I did kNN directly in the next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "gnb_pred=gnb.predict(X_test)\n",
    "round(accuracy_score(y_test, gnb_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(avg_cost(y_test, gnb_pred),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 160, 'num_leaves': 14}\n",
      "Optimal Estimator:  LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=160, n_jobs=-1, num_leaves=14, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "Accuracy:  0.9556521739130435\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "import lightgbm as lgbm\n",
    "parameters = {'learning_rate':[0.05,0.03,0.1],'n_estimators':[100,120,140,160],'num_leaves':[12,14,16,18,20,31]}\n",
    "gs_lgbm = GridSearchCV(lgbm.LGBMClassifier(), parameters, scoring='accuracy',cv=5, n_jobs=4)\n",
    "gs_lgbm = gs_lgbm.fit(X_train, y_train)\n",
    "print(gs_lgbm.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lgbm.best_estimator_)\n",
    "print(\"Accuracy: \", gs_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9592"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lgbm_pred = gs_lgbm.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_lgbm_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03, 'n_estimators': 100, 'num_leaves': 18}\n",
      "Optimal Estimator:  LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.03, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=100, n_jobs=-1, num_leaves=18, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "AvgCost:  -0.2008695652173913\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "import lightgbm as lgbm\n",
    "parameters = {'learning_rate':[0.05,0.03,0.1],'n_estimators':[100,120,140,160],'num_leaves':[12,14,16,18,20,31]}\n",
    "gs_lgbm2 = GridSearchCV(lgbm.LGBMClassifier(), parameters, scoring=avg_cost_score,cv=5, n_jobs=4)\n",
    "gs_lgbm2 = gs_lgbm2.fit(X_train, y_train)\n",
    "print(gs_lgbm2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lgbm2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_lgbm2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2745"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lgbm_pred2 = gs_lgbm2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_lgbm_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'loss': 'hinge', 'tol': 0.0001}\n",
      "Optimal Estimator:  LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=29, tol=0.0001, verbose=0)\n",
      "Accuracy:  0.9005797101449275\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.svm import LinearSVC\n",
    "gs_svm = GridSearchCV(estimator=LinearSVC(random_state=29),\n",
    "                  param_grid=[{'loss':['hinge','squared_hinge'], \n",
    "                              'C':[1,0.1,0.01],\n",
    "                              'tol':[1e-4,1e-5,1e-6]}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_svm = gs_svm.fit(X_train, y_train)\n",
    "print(gs_svm.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_svm.best_estimator_)\n",
    "print(\"Accuracy: \", gs_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm_pred = gs_svm.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_svm_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'loss': 'hinge', 'tol': 0.0001}\n",
      "Optimal Estimator:  LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=29, tol=0.0001, verbose=0)\n",
      "AvgCost:  -0.4101449275362319\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.svm import LinearSVC\n",
    "gs_svm2 = GridSearchCV(estimator=LinearSVC(random_state=29),\n",
    "                  param_grid=[{'loss':['hinge','squared_hinge'], \n",
    "                              'C':[1,0.1,0.01],\n",
    "                              'tol':[1e-4,1e-5,1e-6]}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_svm2 = gs_svm2.fit(X_train, y_train)\n",
    "print(gs_svm2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_svm2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_svm2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3997"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm_pred2 = gs_svm2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_svm_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 8, 'n_estimators': 20}\n",
      "Optimal Estimator:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=8,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy:  0.9249275362318841\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators':[10,20,30],'max_depth':[2,3,4,5],\"criterion\":[\"gini\", \"entropy\"],\"min_samples_split\":[2,4,8]}\n",
    "gs_forest = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring='accuracy',n_jobs=4)\n",
    "gs_forest= gs_forest.fit(X_train, y_train)\n",
    "print(gs_forest.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_forest.best_estimator_)\n",
    "print(\"Accuracy: \", gs_forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9149"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_forest_pred = gs_forest.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_forest_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 4, 'n_estimators': 30}\n",
      "Optimal Estimator:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "AvgCost:  -0.18927536231884057\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators':[10,20,30],'max_depth':[2,3,4,5],\"criterion\":[\"gini\", \"entropy\"],\"min_samples_split\":[2,4,8]}\n",
    "gs_forest2 = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring=avg_cost_score,n_jobs=4)\n",
    "gs_forest2 = gs_forest2.fit(X_train, y_train)\n",
    "print(gs_forest2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_forest2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_forest2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2884"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_forest_pred2 = gs_forest2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_forest_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1'}\n",
      "Optimal Estimator:  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=29, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy:  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "gs_lr = GridSearchCV(estimator=LogisticRegression(random_state=29),\n",
    "                  param_grid=[{'C': [  0.001, 0.01, 0.1 ,1 ,10 ,100, 1000],\n",
    "                             'penalty':['l1','l2']}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, n_jobs=4)\n",
    "gs_lr = gs_lr.fit(X_train,y_train)\n",
    "print(gs_lr.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lr.best_estimator_)\n",
    "print(\"Accuracy: \", gs_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_pred = gs_lr.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_lr_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1'}\n",
      "Optimal Estimator:  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=29, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "AvgCost:  -0.33942028985507244\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "gs_lr2 = GridSearchCV(estimator=LogisticRegression(random_state=29),\n",
    "                  param_grid=[{'C': [  0.001, 0.01, 0.1 ,1 ,10 ,100, 1000],\n",
    "                             'penalty':['l1','l2']}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5, n_jobs=4)\n",
    "gs_lr2 = gs_lr2.fit(X_train,y_train)\n",
    "print(gs_lr2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lr2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_lr2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2989"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_pred2 = gs_lr2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_lr_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9348"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at potential NN model firstly\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes = (100,100,100,100), learning_rate = 'adaptive', random_state = 29)\n",
    "nn = nn.fit(X_train, y_train)\n",
    "nn_pred = nn.predict(X_test)\n",
    "round(accuracy_score(y_test, nn_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0002, 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Optimal Estimator:  MLPClassifier(activation='relu', alpha=0.0002, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.002, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=29, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Accuracy:  0.927536231884058\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "parameters = {'learning_rate_init': [0.001,0.002],\n",
    "              'alpha': [0,0.0001,0.0002],\n",
    "              'solver': ['lbfgs', 'adam']}\n",
    "gs_nn = GridSearchCV(MLPClassifier(hidden_layer_sizes = (100,100,100,100),learning_rate = 'constant',random_state = 29), parameters, cv=5, n_jobs=4)\n",
    "gs_nn = gs_nn.fit(X_train, y_train)\n",
    "print(gs_nn.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_nn.best_estimator_)\n",
    "print(\"Accuracy: \", gs_nn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nn_pred = gs_nn.predict(X_test) \n",
    "round(accuracy_score(y_test, gs_nn_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0002, 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Optimal Estimator:  MLPClassifier(activation='relu', alpha=0.0002, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.002, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=29, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "AvgCost:  -0.35681159420289854\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "parameters = {'learning_rate_init': [0.001,0.002],\n",
    "              'alpha': [0,0.0001,0.0002],\n",
    "              'solver': ['lbfgs', 'adam']}\n",
    "gs_nn2 = GridSearchCV(MLPClassifier(hidden_layer_sizes = (100,100,100,100),learning_rate = 'constant',random_state = 29), parameters,\n",
    "                     scoring=avg_cost_score,cv=5, n_jobs=4)\n",
    "gs_nn2 = gs_nn2.fit(X_train, y_train)\n",
    "print(gs_nn2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_nn2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_nn2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3501"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nn_pred2 = gs_nn2.predict(X_test) \n",
    "round(avg_cost(y_test, gs_nn_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base on three models that perform relatively better: Neural Network, lightGBM, random forest, with the best parameters - based on accuracy\n",
    "models = [gs_nn.best_estimator_,gs_lgbm.best_estimator_,gs_forest.best_estimator_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [MLPClassifier]\n",
      "    fold  0:  [0.93279258]\n",
      "    fold  1:  [0.90961761]\n",
      "    fold  2:  [0.92227378]\n",
      "    fold  3:  [0.91995360]\n",
      "    ----\n",
      "    MEAN:     [0.92115939] + [0.00823468]\n",
      "    FULL:     [0.92115942]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "    fold  0:  [0.95365006]\n",
      "    fold  1:  [0.94785632]\n",
      "    fold  2:  [0.93619490]\n",
      "    fold  3:  [0.95707657]\n",
      "    ----\n",
      "    MEAN:     [0.94869446] + [0.00793346]\n",
      "    FULL:     [0.94869565]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.92004635]\n",
      "    fold  1:  [0.92352260]\n",
      "    fold  2:  [0.90255220]\n",
      "    fold  3:  [0.93735499]\n",
      "    ----\n",
      "    MEAN:     [0.92086903] + [0.01239971]\n",
      "    FULL:     [0.92086957]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(models,X_train, y_train, X_test,regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None, \n",
    "                           metric=accuracy_score,n_folds=4,stratified=True,shuffle=True, random_state=29,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model = LogisticRegression(C=1,penalty=\"l1\")\n",
    "stacking_model = stacking_model.fit(S_train, y_train)\n",
    "y_pred_s = stacking_model.predict(S_test)\n",
    "round(accuracy_score(y_test, y_pred_s),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base on three models that perform relatively better: decision tree, lightGBM, random forest, with the best parameters - based on avg classification cost\n",
    "models2 = [gs_tree2.best_estimator_,gs_lgbm2.best_estimator_,gs_forest2.best_estimator_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [DecisionTreeClassifier]\n",
      "    fold  0:  [0.90730012]\n",
      "    fold  1:  [0.90961761]\n",
      "    fold  2:  [0.90951276]\n",
      "    fold  3:  [0.92691415]\n",
      "    ----\n",
      "    MEAN:     [0.91333616] + [0.00789370]\n",
      "    FULL:     [0.91333333]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "    fold  0:  [0.95017381]\n",
      "    fold  1:  [0.93395133]\n",
      "    fold  2:  [0.92227378]\n",
      "    fold  3:  [0.95939675]\n",
      "    ----\n",
      "    MEAN:     [0.94144892] + [0.01433656]\n",
      "    FULL:     [0.94144928]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.90266512]\n",
      "    fold  1:  [0.90730012]\n",
      "    fold  2:  [0.89443155]\n",
      "    fold  3:  [0.91647332]\n",
      "    ----\n",
      "    MEAN:     [0.90521753] + [0.00796684]\n",
      "    FULL:     [0.90521739]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(models2,X_train, y_train, X_test,regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None, \n",
    "                           metric=accuracy_score,n_folds=4,stratified=True,shuffle=True, random_state=29,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2407"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model = LogisticRegression(C=1,penalty=\"l1\")\n",
    "stacking_model = stacking_model.fit(S_train, y_train)\n",
    "y_pred_s2 = stacking_model.predict(S_test)\n",
    "round(avg_cost(y_test, y_pred_s2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models with preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "sc.fit(X)\n",
    "Xnormal = sc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Chi-square test to choose attributes\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_new=SelectKBest(chi2, k=40).fit_transform(Xnormal, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train/Test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.25, random_state=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "Optimal Estimator:  DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
      "            splitter='best')\n",
      "Accuracy:  0.923768115942029\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "gs_tree = GridSearchCV(estimator=DecisionTreeClassifier(random_state=29),\n",
    "                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], 'criterion':['gini','entropy'], \n",
    "                              'min_samples_leaf':[1,2,3,4,5],\n",
    "                              'min_samples_split':[2,3,4,5]}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_tree = gs_tree.fit(X_train, y_train)\n",
    "print(gs_tree.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_tree.best_estimator_)\n",
    "print(\"Accuracy: \", gs_tree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "gs_tree_pred = gs_tree.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_tree_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Optimal Estimator:  DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
      "            splitter='best')\n",
      "AvgCost:  -0.2678260869565217\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "gs_tree2 = GridSearchCV(estimator=DecisionTreeClassifier(random_state=29),\n",
    "                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None], 'criterion':['gini','entropy'], \n",
    "                              'min_samples_leaf':[1,2,3,4,5],\n",
    "                              'min_samples_split':[2,3,4,5]}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_tree2 = gs_tree2.fit(X_train, y_train)\n",
    "print(gs_tree2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_tree2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_tree2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "gs_tree_pred2 = gs_tree2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_tree_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 7, 'weights': 'distance'}\n",
      "Optimal Estimator:  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "           weights='distance')\n",
      "Accuracy:  0.898840579710145\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn import neighbors\n",
    "gs_knn = GridSearchCV(estimator=neighbors.KNeighborsClassifier(p=2, \n",
    "                           metric='minkowski'),\n",
    "                  param_grid=[{'n_neighbors': [1,3,5,7,9],\n",
    "                               'weights':['uniform','distance']}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "\n",
    "gs_knn = gs_knn.fit(X_train, y_train)\n",
    "print(gs_knn.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_knn.best_estimator_)\n",
    "print(\"Accuracy: \", gs_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "gs_knn_pred = gs_knn.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_knn_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 7, 'weights': 'distance'}\n",
      "Optimal Estimator:  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
      "           weights='distance')\n",
      "AvgCost:  0.898840579710145\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn import neighbors\n",
    "gs_knn2 = GridSearchCV(estimator=neighbors.KNeighborsClassifier(p=2, \n",
    "                           metric='minkowski'),\n",
    "                  param_grid=[{'n_neighbors': [1,3,5,7,9],\n",
    "                               'weights':['uniform','distance']}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "\n",
    "gs_knn2 = gs_knn.fit(X_train, y_train)\n",
    "print(gs_knn2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_knn2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_knn2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4144"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result on test\n",
    "gs_knn_pred2 = gs_knn2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_knn_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "gnb_pred=gnb.predict(X_test)\n",
    "round(accuracy_score(y_test, gnb_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0182"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(avg_cost(y_test, gnb_pred),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'n_estimators': 140, 'num_leaves': 31}\n",
      "Optimal Estimator:  LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.05, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=140, n_jobs=-1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "Accuracy:  0.951304347826087\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "import lightgbm as lgbm\n",
    "parameters = {'learning_rate':[0.05,0.03,0.1],'n_estimators':[100,120,140,160],'num_leaves':[12,14,16,18,20,31]}\n",
    "gs_lgbm = GridSearchCV(lgbm.LGBMClassifier(), parameters, scoring='accuracy',cv=5, n_jobs=4)\n",
    "gs_lgbm = gs_lgbm.fit(X_train, y_train)\n",
    "print(gs_lgbm.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lgbm.best_estimator_)\n",
    "print(\"Accuracy: \", gs_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9522"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lgbm_pred = gs_lgbm.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_lgbm_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03, 'n_estimators': 120, 'num_leaves': 31}\n",
      "Optimal Estimator:  LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.03, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=120, n_jobs=-1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "AvgCost:  -0.2281159420289855\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "import lightgbm as lgbm\n",
    "parameters = {'learning_rate':[0.05,0.03,0.1],'n_estimators':[100,120,140,160],'num_leaves':[12,14,16,18,20,31]}\n",
    "gs_lgbm2 = GridSearchCV(lgbm.LGBMClassifier(), parameters, scoring=avg_cost_score,cv=5, n_jobs=4)\n",
    "gs_lgbm2 = gs_lgbm2.fit(X_train, y_train)\n",
    "print(gs_lgbm2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lgbm2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_lgbm2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.252"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lgbm_pred2 = gs_lgbm2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_lgbm_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'loss': 'squared_hinge', 'tol': 0.0001}\n",
      "Optimal Estimator:  LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=29, tol=0.0001,\n",
      "     verbose=0)\n",
      "Accuracy:  0.907536231884058\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.svm import LinearSVC\n",
    "gs_svm = GridSearchCV(estimator=LinearSVC(random_state=29),\n",
    "                  param_grid=[{'loss':['hinge','squared_hinge'], \n",
    "                              'C':[1,0.1,0.01],\n",
    "                              'tol':[1e-4,1e-5,1e-6]}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_svm = gs_svm.fit(X_train, y_train)\n",
    "print(gs_svm.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_svm.best_estimator_)\n",
    "print(\"Accuracy: \", gs_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8949"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm_pred = gs_svm.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_svm_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'loss': 'squared_hinge', 'tol': 0.0001}\n",
      "Optimal Estimator:  LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=29, tol=0.0001,\n",
      "     verbose=0)\n",
      "AvgCost:  -0.3507246376811594\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.svm import LinearSVC\n",
    "gs_svm2 = GridSearchCV(estimator=LinearSVC(random_state=29),\n",
    "                  param_grid=[{'loss':['hinge','squared_hinge'], \n",
    "                              'C':[1,0.1,0.01],\n",
    "                              'tol':[1e-4,1e-5,1e-6]}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5,\n",
    "                  n_jobs=4)\n",
    "gs_svm2 = gs_svm2.fit(X_train, y_train)\n",
    "print(gs_svm2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_svm2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_svm2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4023"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm_pred2 = gs_svm2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_svm_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 8, 'n_estimators': 30}\n",
      "Optimal Estimator:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=8,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy:  0.9243478260869565\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators':[10,20,30],'max_depth':[2,3,4,5],\"criterion\":[\"gini\", \"entropy\"],\"min_samples_split\":[2,4,8]}\n",
    "gs_forest = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring='accuracy',n_jobs=4)\n",
    "gs_forest= gs_forest.fit(X_train, y_train)\n",
    "print(gs_forest.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_forest.best_estimator_)\n",
    "print(\"Accuracy: \", gs_forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9149"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_forest_pred = gs_forest.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_forest_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 30}\n",
      "Optimal Estimator:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "AvgCost:  -0.1991304347826087\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators':[10,20,30],'max_depth':[2,3,4,5],\"criterion\":[\"gini\", \"entropy\"],\"min_samples_split\":[2,4,8]}\n",
    "gs_forest2 = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring=avg_cost_score,n_jobs=4)\n",
    "gs_forest2 = gs_forest2.fit(X_train, y_train)\n",
    "print(gs_forest2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_forest2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_forest2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2693"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_forest_pred2 = gs_forest2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_forest_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000, 'penalty': 'l1'}\n",
      "Optimal Estimator:  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=29, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy:  0.9257971014492754\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "gs_lr = GridSearchCV(estimator=LogisticRegression(random_state=29),\n",
    "                  param_grid=[{'C': [  0.001, 0.01, 0.1 ,1 ,10 ,100, 1000],\n",
    "                             'penalty':['l1','l2']}],\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, n_jobs=4)\n",
    "gs_lr = gs_lr.fit(X_train,y_train)\n",
    "print(gs_lr.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lr.best_estimator_)\n",
    "print(\"Accuracy: \", gs_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_pred = gs_lr.predict(X_test)\n",
    "round(accuracy_score(y_test, gs_lr_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000, 'penalty': 'l1'}\n",
      "Optimal Estimator:  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=29, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "AvgCost:  -0.3220289855072464\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "gs_lr2 = GridSearchCV(estimator=LogisticRegression(random_state=29),\n",
    "                  param_grid=[{'C': [  0.001, 0.01, 0.1 ,1 ,10 ,100, 1000],\n",
    "                             'penalty':['l1','l2']}],\n",
    "                  scoring=avg_cost_score,\n",
    "                  cv=5, n_jobs=4)\n",
    "gs_lr2 = gs_lr2.fit(X_train,y_train)\n",
    "print(gs_lr2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_lr2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_lr2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3736"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_pred2 = gs_lr2.predict(X_test)\n",
    "round(avg_cost(y_test, gs_lr_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at potential NN model firstly\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes = (100,100,100,100), learning_rate = 'adaptive', random_state = 29)\n",
    "nn = nn.fit(X_train, y_train)\n",
    "nn_pred = nn.predict(X_test)\n",
    "round(accuracy_score(y_test, nn_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Optimal Estimator:  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.002, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=29, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Accuracy:  0.9359420289855073\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on accuracy\n",
    "parameters = {'learning_rate_init': [0.001,0.002],\n",
    "              'alpha': [0,0.0001,0.0002],\n",
    "              'solver': ['lbfgs', 'adam']}\n",
    "gs_nn = GridSearchCV(MLPClassifier(hidden_layer_sizes = (100,100,100,100),learning_rate = 'constant',random_state = 29), parameters, cv=5, n_jobs=4)\n",
    "gs_nn = gs_nn.fit(X_train, y_train)\n",
    "print(gs_nn.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_nn.best_estimator_)\n",
    "print(\"Accuracy: \", gs_nn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nn_pred = gs_nn.predict(X_test) \n",
    "round(accuracy_score(y_test, gs_nn_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Optimal Estimator:  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
      "       learning_rate_init=0.002, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=29, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "AvgCost:  -0.3327536231884058\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to choose the best parameters based on average misclassification cost\n",
    "parameters = {'learning_rate_init': [0.001,0.002],\n",
    "              'alpha': [0,0.0001,0.0002],\n",
    "              'solver': ['lbfgs', 'adam']}\n",
    "gs_nn2 = GridSearchCV(MLPClassifier(hidden_layer_sizes = (100,100,100,100),learning_rate = 'constant',random_state = 29), parameters,\n",
    "                     scoring=avg_cost_score,cv=5, n_jobs=4)\n",
    "gs_nn2 = gs_nn2.fit(X_train, y_train)\n",
    "print(gs_nn2.best_params_)\n",
    "print(\"Optimal Estimator: \", gs_nn2.best_estimator_)\n",
    "print(\"AvgCost: \", gs_nn2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2737"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nn_pred2 = gs_nn2.predict(X_test) \n",
    "round(avg_cost(y_test, gs_nn_pred2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base on three models that perform relatively better: Neural Network, lightGBM, decision tree, with the best parameters - based on accuracy\n",
    "models = [gs_nn.best_estimator_,gs_lgbm.best_estimator_,gs_tree.best_estimator_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [MLPClassifier]\n",
      "    fold  0:  [0.93511008]\n",
      "    fold  1:  [0.91772885]\n",
      "    fold  2:  [0.93039443]\n",
      "    fold  3:  [0.94663573]\n",
      "    ----\n",
      "    MEAN:     [0.93246727] + [0.01035914]\n",
      "    FULL:     [0.93246377]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "    fold  0:  [0.95480881]\n",
      "    fold  1:  [0.93858633]\n",
      "    fold  2:  [0.94779582]\n",
      "    fold  3:  [0.95823666]\n",
      "    ----\n",
      "    MEAN:     [0.94985690] + [0.00751691]\n",
      "    FULL:     [0.94985507]\n",
      "\n",
      "model  2:     [DecisionTreeClassifier]\n",
      "    fold  0:  [0.92004635]\n",
      "    fold  1:  [0.91077636]\n",
      "    fold  2:  [0.90951276]\n",
      "    fold  3:  [0.93155452]\n",
      "    ----\n",
      "    MEAN:     [0.91797250] + [0.00883351]\n",
      "    FULL:     [0.91797101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(models,X_train, y_train, X_test,regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None, \n",
    "                           metric=accuracy_score,n_folds=4,stratified=True,shuffle=True, random_state=29,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9513"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model = LogisticRegression(C=1,penalty=\"l1\")\n",
    "stacking_model = stacking_model.fit(S_train, y_train)\n",
    "y_pred_s = stacking_model.predict(S_test)\n",
    "round(accuracy_score(y_test, y_pred_s),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base on three models that perform relatively better: decision tree, lightGBM, random forest, with the best parameters - based on avg classification cost\n",
    "models2 = [gs_tree2.best_estimator_,gs_lgbm2.best_estimator_,gs_forest2.best_estimator_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [DecisionTreeClassifier]\n",
      "    fold  0:  [0.91657010]\n",
      "    fold  1:  [0.89918888]\n",
      "    fold  2:  [0.90719258]\n",
      "    fold  3:  [0.92575406]\n",
      "    ----\n",
      "    MEAN:     [0.91217640] + [0.00996458]\n",
      "    FULL:     [0.91217391]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "    fold  0:  [0.95365006]\n",
      "    fold  1:  [0.93395133]\n",
      "    fold  2:  [0.94083527]\n",
      "    fold  3:  [0.95823666]\n",
      "    ----\n",
      "    MEAN:     [0.94666833] + [0.00972519]\n",
      "    FULL:     [0.94666667]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.92815759]\n",
      "    fold  1:  [0.91541136]\n",
      "    fold  2:  [0.90603248]\n",
      "    fold  3:  [0.93735499]\n",
      "    ----\n",
      "    MEAN:     [0.92173910] + [0.01195608]\n",
      "    FULL:     [0.92173913]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(models2,X_train, y_train, X_test,regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None, \n",
    "                           metric=accuracy_score,n_folds=4,stratified=True,shuffle=True, random_state=29,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2328"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model = LogisticRegression(C=1,penalty=\"l1\")\n",
    "stacking_model = stacking_model.fit(S_train, y_train)\n",
    "y_pred_s2 = stacking_model.predict(S_test)\n",
    "round(avg_cost(y_test, y_pred_s2),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model based on overall predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After trying different models and selecting the best parameters based on transformed and untranformed data,\n",
    " I found out that lightGBM (accuracy=0.9592) and stacking (0.9574) has very close results (both on untransformed data).\n",
    " Thus I decided to look at their precision, recall, f-measure, ROC curve to choose the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best two models\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X, y, test_size=0.25, random_state=29)\n",
    "\n",
    "# lightGBM\n",
    "lgbm_f=lgbm.LGBMClassifier(learning_rate=0.1, n_estimators=160, num_leaves=14)\n",
    "lgbm_f= lgbm_f.fit(X_train_f, y_train_f)\n",
    "lgbm_pred_f = lgbm_f.predict(X_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       680\n",
      "           1       0.95      0.95      0.95       471\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1151\n",
      "   macro avg       0.96      0.96      0.96      1151\n",
      "weighted avg       0.96      0.96      0.96      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report\n",
    "print(classification_report(y_test_f, lgbm_pred_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_f = [MLPClassifier(hidden_layer_sizes = (100,100,100,100),learning_rate = 'constant',random_state = 29,alpha=0.0002, learning_rate_init=0.002, solver='adam'),\n",
    "            lgbm.LGBMClassifier(learning_rate=0.1, n_estimators=160, num_leaves=14),\n",
    "            RandomForestClassifier(criterion='entropy', max_depth=5, min_samples_split=8, n_estimators=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [MLPClassifier]\n",
      "    fold  0:  [0.93279258]\n",
      "    fold  1:  [0.90961761]\n",
      "    fold  2:  [0.92227378]\n",
      "    fold  3:  [0.91995360]\n",
      "    ----\n",
      "    MEAN:     [0.92115939] + [0.00823468]\n",
      "    FULL:     [0.92115942]\n",
      "\n",
      "model  1:     [LGBMClassifier]\n",
      "    fold  0:  [0.95365006]\n",
      "    fold  1:  [0.94785632]\n",
      "    fold  2:  [0.93619490]\n",
      "    fold  3:  [0.95707657]\n",
      "    ----\n",
      "    MEAN:     [0.94869446] + [0.00793346]\n",
      "    FULL:     [0.94869565]\n",
      "\n",
      "model  2:     [RandomForestClassifier]\n",
      "    fold  0:  [0.92699884]\n",
      "    fold  1:  [0.92004635]\n",
      "    fold  2:  [0.91299304]\n",
      "    fold  3:  [0.94199536]\n",
      "    ----\n",
      "    MEAN:     [0.92550840] + [0.01072974]\n",
      "    FULL:     [0.92550725]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train_f, S_test_f = stacking(models_f,X_train_f, y_train_f, X_test_f,\n",
    "                               regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None, \n",
    "                           metric=accuracy_score,n_folds=4,stratified=True,shuffle=True, random_state=29,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model_f = LogisticRegression(C=1,penalty=\"l1\")\n",
    "stacking_model_f = stacking_model_f.fit(S_train_f, y_train_f)\n",
    "y_pred_s_f = stacking_model_f.predict(S_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       680\n",
      "           1       0.94      0.94      0.94       471\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1151\n",
      "   macro avg       0.95      0.95      0.95      1151\n",
      "weighted avg       0.95      0.95      0.95      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_f, y_pred_s_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC & AUC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC curve\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn import  metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_f, lgbm_pred_f)\n",
    "plt.plot(fpr, tpr,label='lightGBM  '+ str(round(auc(x=fpr, y=tpr),3)),linestyle='--')\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(y_test_f, y_pred_s_f)\n",
    "plt.plot(fpr2, tpr2,label='Stacking  '+ str(round(auc(x=fpr2, y=tpr2),3)),linestyle='-.')\n",
    "plt.plot([0, 1], [0, 1], # Visualize random classifier\n",
    "         linestyle=':',\n",
    "         color='gray',\n",
    "         linewidth=2)\n",
    "plt.legend(loc='lower right')  \n",
    "plt.xlabel('False positive rate (FPR)')\n",
    "plt.ylabel('True positive rate (TPR)')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary on best model based on accuracy rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the precision, recall and f-measure, lightGBM always performs better than stacking. It is also true when we look at ROC curve . Thus, I decided to choose lightGBM as the best model based on accuracy score (performance on test set is 0.9592)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model based on average misclassification cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all models I tried, the best model on average misclassification cost is using stacking method transformed data, with the average misclassification cost of 0.2328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       680\n",
      "           1       0.95      0.94      0.94       471\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1151\n",
      "   macro avg       0.95      0.95      0.95      1151\n",
      "weighted avg       0.95      0.95      0.95      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See precision, recall and f-measure\n",
    "print(classification_report(y_test, y_pred_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(y_test, y_pred_s2)\n",
    "plt.plot(fpr3, tpr3,label='Stacking  '+ str(round(auc(x=fpr3, y=tpr3),3)),linestyle='--')\n",
    "plt.legend(loc='lower right')  \n",
    "plt.xlabel('False positive rate (FPR)')\n",
    "plt.ylabel('True positive rate (TPR)')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
